{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f62c52",
   "metadata": {},
   "source": [
    "# Model fine tuning test\n",
    "#### Local test (CPU only),\n",
    "(Model only trained for 20 steps, so the LoRA model memorizes a few examples and nothing more, not generalize yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83d29b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install torch\n",
    "%pip install datasets\n",
    "%pip install transformers\n",
    "%pip install peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc556059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22591266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EXCEL_FILE = r\"../../data/mccray/completion_df.xlsx\"  # Replace with your file path\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama_v1.1\" \n",
    "# MODEL_NAME = \"distilgpt2\" \n",
    "# MODEL_NAME = \"microsoft/DialoGPT-small2\" # Alternative for slightly better quality\n",
    "OUTPUT_DIR = \"./400-completion-tasks-distilGPT2\"\n",
    "output_dir = OUTPUT_DIR \n",
    "MAX_LENGTH = 256         # Shorter for CPU efficiency\n",
    "MIN_TRANSCRIPT_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d413190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(excel_file):\n",
    "    \"\"\"Load Excel file and preprocess the data\"\"\"\n",
    "    print(\"Loading Excel file...\")\n",
    "    df = pd.read_excel(excel_file)\n",
    "    \n",
    "    print(f\"Dataset size: {len(df)}\")\n",
    "    \n",
    "    # Create training format\n",
    "    def format_example(input, output):\n",
    "        prompt = f\"### INPUT:{input}\\n### RESPONSE:\"\n",
    "        response = output  \n",
    "        return f\"{prompt} {response}\"\n",
    "\n",
    "        print(df['input'])\n",
    "        print(df['output'])\n",
    "    \n",
    "    df['text'] = df.apply(lambda row: format_example(row['input'], row['output']), axis=1)\n",
    "    \n",
    "    return df[['text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323f583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df):\n",
    "    \"\"\"Convert DataFrame to HuggingFace Dataset\"\"\"\n",
    "    print(\"Creating HuggingFace Dataset...\")\n",
    "    print(df)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_tokenizer(model_name):\n",
    "    \"\"\"Load model and tokenizer for CPU training\"\"\"\n",
    "    print(f\"Loading model and tokenizer: {model_name}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load model for CPU\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float32,  # Use float32 for CPU\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def setup_lora(model):\n",
    "    \"\"\"Configure LoRA for the model\"\"\"\n",
    "    print(\"Setting up LoRA...\")\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=4,  # Even lower rank for CPU training\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"c_attn\", \"c_proj\"],  # For GPT-2 based models\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer):\n",
    "    \"\"\"Tokenize the dataset and add labels for Causal LM\"\"\"\n",
    "    print(\"Tokenizing dataset...\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        tokens = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_LENGTH,\n",
    "        )\n",
    "        # Add labels = input_ids for causal LM loss\n",
    "        tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "        return tokens\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af0c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, tokenizer, train_dataset):\n",
    "    \"\"\"Fine-tune the model using Trainer\"\"\"\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=5,      # Try 3 or more epochs\n",
    "        max_steps=1000,           # Or set higher steps, e.g., 100, 500, 1000\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=2,\n",
    "        warmup_steps=5,\n",
    "        learning_rate=1e-3,\n",
    "        logging_steps=2,\n",
    "        save_strategy=\"epoch\",\n",
    "        eval_strategy=\"no\",\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_num_workers=0,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the model\n",
    "    print(\"Saving model...\")\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a0c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_generation(model, tokenizer):\n",
    "    \"\"\"Test the fine-tuned model with a sample prompt\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TESTING GENERATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    test_prompt = \"\"\"### INPUT: Give me the entire transcript, including what is missing, from this ending portion of a McCray transcript: ... the expression of your views with respect to the very controversial Civil Rights Message of the President. Sincerely, Mr. H. McCray Columbia, South Carolina\n",
    "### RESPONSE:\"\"\"\n",
    "    \n",
    "    print(f\"Input prompt:\\n{test_prompt}\\n\")\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate response\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=50,  # Shorter for CPU\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode and print\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response_start = generated_text.find(\"### RESPONSE:\") + len(\"### RESPONSE:\")\n",
    "    response = generated_text[response_start:].strip()\n",
    "    \n",
    "    print(f\"Generated response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686936e0",
   "metadata": {},
   "source": [
    "## Model code ran here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ce0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    print(\"Starting LoRA Fine-tuning Pipeline\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        # 1. Load and preprocess data\n",
    "        df = load_and_preprocess_data(EXCEL_FILE)\n",
    "        \n",
    "        # 2. Create dataset\n",
    "        dataset = create_dataset(df)\n",
    "        \n",
    "        # 3. Setup model and tokenizer\n",
    "        model, tokenizer = setup_model_and_tokenizer(MODEL_NAME)\n",
    "        \n",
    "        # 4. Setup LoRA\n",
    "        model = setup_lora(model)\n",
    "        \n",
    "        # 5. Tokenize dataset\n",
    "        train_dataset = tokenize_dataset(dataset, tokenizer)\n",
    "        \n",
    "        # 6. Train model\n",
    "        trainer = train_model(model, tokenizer, train_dataset)\n",
    "        \n",
    "        # 7. Test generation\n",
    "        test_generation(model, tokenizer)\n",
    "        \n",
    "        print(f\"\\nTraining completed! Model saved to: {OUTPUT_DIR}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install torch transformers datasets peft pandas openpyxl\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25905ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "\n",
    "# Load PEFT config\n",
    "peft_config = PeftConfig.from_pretrained(output_dir)\n",
    "\n",
    "# Load base model and tokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Attach LoRA weights\n",
    "model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "\n",
    "# Set model to eval mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"\"\"### INPUT: Based on the year, 1946, and title, \"Memorandum to executive officers of the Progressive Democratic Party, Page 2\", complete this transcript:\n",
    "(Page Two) covered in 8 full report to ell executive end unit officers we hope to submit within the next ton days. But it writes \"finish\" to the several decades of unfortunate political operations which, principally among Southern States, have make 8 great mockery of the very word \"Democracy\"....\n",
    "### RESPONSE:\"\"\"\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6521afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Models can be pushed to huggingface when we have a model that we want to do further testing on\n",
    "\n",
    "from huggingface_hub import login\n",
    "from peft import PeftModel\n",
    "\n",
    "model.push_to_hub(\"your-username/your-model-name\")\n",
    "tokenizer.push_to_hub(\"your-username/your-model-name\")\n",
    "\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_aspire)",
   "language": "python",
   "name": "venv_aspire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
