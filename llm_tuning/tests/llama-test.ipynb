{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ed1aa4",
   "metadata": {},
   "source": [
    "# Llama Tuning Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af4109a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/meta-llama/Llama-3.2-1B\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a814a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d4783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1. Load your dataframe: df = pd.read_csv('your_file.csv')\")\n",
    "print(\"2. Load transcript data: train_data = load_transcript_data_from_dataframe(df)\")\n",
    "print(\"3. Create dataset: train_dataset = Dataset.from_list(train_data)\")\n",
    "print(\"4. Tokenize: tokenized_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\")\n",
    "print(\"5. Uncomment the start_training() call\")\n",
    "print(\"6. Monitor training progress and adjust batch size if needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace60e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add padding token if not present (common with Llama models)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Data loading functions\n",
    "def load_transcript_data_from_excel(file_path, column_name='training', sheet_name=0):\n",
    "    \"\"\"Load transcript data from Excel file\"\"\"\n",
    "    # Read Excel file into DataFrame\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    \n",
    "    # Convert specified column to list of dictionaries\n",
    "    data = []\n",
    "    for text in df[column_name].values:\n",
    "        # Skip empty/null values\n",
    "        if pd.notna(text) and str(text).strip():\n",
    "            data.append({\"text\": str(text)})\n",
    "    \n",
    "    print(f\"Loaded {len(data)} transcripts from Excel file\")\n",
    "    return data\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the text data with appropriate truncation for short transcripts\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,  # Dynamic padding handled by data collator\n",
    "        max_length=512,  # Reasonable for short transcripts, adjust if needed\n",
    "        return_special_tokens_mask=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e5c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare dataset\n",
    "# Example usage:\n",
    "# df = pd.read_csv('your_data.csv')  # or pd.read_pickle(), pd.read_parquet(), etc.\n",
    "# train_data = load_transcript_data_from_dataframe(df)\n",
    "# train_dataset = Dataset.from_list(train_data)\n",
    "# tokenized_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM, not masked LM\n",
    "    pad_to_multiple_of=8  # Slight efficiency improvement\n",
    ")\n",
    "\n",
    "# Training arguments optimized for CPU and small dataset\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama-finetuned\",\n",
    "    \n",
    "    # Small dataset optimization\n",
    "    num_train_epochs=3,  # More epochs for small dataset to ensure convergence\n",
    "    per_device_train_batch_size=1,  # Small batch size for CPU training\n",
    "    gradient_accumulation_steps=8,  # Simulate larger batch (effective batch size = 8)\n",
    "    \n",
    "    # Learning rate settings\n",
    "    learning_rate=2e-5,  # Slightly higher for small dataset, but conservative for stability\n",
    "    warmup_steps=50,  # ~10% of total steps for gradual learning rate increase\n",
    "    lr_scheduler_type=\"cosine\",  # Smooth learning rate decay\n",
    "    \n",
    "    # CPU-specific optimizations\n",
    "    dataloader_num_workers=2,  # Moderate for CPU, adjust based on your cores\n",
    "    fp16=False,  # CPU doesn't support fp16\n",
    "    bf16=False,  # Most CPUs don't support bf16\n",
    "    \n",
    "    # Memory and efficiency\n",
    "    max_grad_norm=1.0,  # Gradient clipping for stability\n",
    "    remove_unused_columns=True,  # Memory optimization\n",
    "    dataloader_pin_memory=False,  # CPU training doesn't benefit from this\n",
    "    \n",
    "    # Logging and evaluation\n",
    "    logging_steps=10,  # Frequent logging for small dataset monitoring\n",
    "    eval_strategy=\"steps\" if \"eval_dataset\" in locals() else \"no\",\n",
    "    eval_steps=50,  # Evaluate every 50 steps if eval data available\n",
    "    save_steps=100,  # Save checkpoints periodically\n",
    "    save_total_limit=2,  # Keep only 2 latest checkpoints to save space\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    \n",
    "    # Output control\n",
    "    report_to=None,  # Disable wandb/tensorboard for simplicity\n",
    "    load_best_model_at_end=True if \"eval_dataset\" in locals() else False,\n",
    "    metric_for_best_model=\"eval_loss\" if \"eval_dataset\" in locals() else None,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    # train_dataset=tokenized_dataset,  # Uncomment when you have data loaded\n",
    "    # eval_dataset=eval_dataset,  # Add if you have evaluation data\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d571f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training function\n",
    "def start_training():\n",
    "    \"\"\"Start the fine-tuning process\"\"\"\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "    print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "    \n",
    "    # Start training\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the final model\n",
    "    trainer.save_model(\"./llama-finetuned-final\")\n",
    "    tokenizer.save_pretrained(\"./llama-finetuned-final\")\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "\n",
    "# Memory optimization for CPU training\n",
    "def optimize_for_cpu():\n",
    "    \"\"\"Apply CPU-specific optimizations\"\"\"\n",
    "    # Enable memory efficient attention if available\n",
    "    if hasattr(model.config, 'use_memory_efficient_attention'):\n",
    "        model.config.use_memory_efficient_attention = True\n",
    "    \n",
    "    # Set torch to use single thread for deterministic results on CPU\n",
    "    torch.set_num_threads(1)\n",
    "    \n",
    "    # Enable gradient checkpointing to save memory\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "# Apply optimizations\n",
    "optimize_for_cpu()\n",
    "\n",
    "# Example data loading function - adjust based on your data format\n",
    "def load_transcript_data_from_dataframe(df):\n",
    "    \"\"\"Load transcript data from pandas dataframe with 'training' column\"\"\"\n",
    "    # Convert dataframe column to list of dictionaries\n",
    "    data = []\n",
    "    for text in df['training'].values:\n",
    "        # Skip empty/null values\n",
    "        if pd.notna(text) and text.strip():\n",
    "            data.append({\"text\": text})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4095b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ea6c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_aspire)",
   "language": "python",
   "name": "venv_aspire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
