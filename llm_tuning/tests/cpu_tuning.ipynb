{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f62c52",
   "metadata": {},
   "source": [
    "# Model fine tuning test\n",
    "#### Local test (CPU only), simple casual llm model, code generated with claude\n",
    "(Model only trained for 20 steps, so the LoRA model memorizes a few examples and nothing more, not generalize yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94025ad7",
   "metadata": {},
   "source": [
    "### Prompt: \n",
    "I have an Excel file containing historical document transcripts. Each row has:\n",
    "* Title: the title of the document (some titles repeat),\n",
    "* Transcript: the actual text (usually a paragraph or page from the document).\n",
    "I want to fine-tune casual language model using LoRA in Python, just for a quick test demo, not for final production. Please give me a complete working script that does the following:\n",
    "1. Loads the Excel file using pandas.\n",
    "2. Drops any rows where Title or Transcript is missing or too short.\n",
    "3. Creates a training dataset where:\n",
    "   * The input prompt is: ### TITLE: {Title}\\n### TRANSCRIPT:\\n{Transcript}\\n### RESPONSE:\n",
    "   * The response can just be a copy of the transcript for now. This is a placeholder just to show fine-tuning works.\n",
    "   * These are combined into one text field for casual language modeling.\n",
    "4. Converts the dataset into a Hugging Face Dataset object.\n",
    "5. Loads a model like tiiuae/falcon-7b-instruct or another instruct-tuned casual model.\n",
    "6. Applies LoRA using Hugging Face peft with 4-bit quantization (bitsandbytes) to keep memory low and training fast.\n",
    "7. Tokenizes the dataset using the model's tokenizer.\n",
    "8. Fine-tunes the model using Trainer, training for just 1 epoch with a small batch size.\n",
    "9. Saves the fine-tuned model.\n",
    "10. Runs a single generation test from a sample prompt like: ### TITLE: Lincoln's Gettysburg Address\n",
    "***###*** TRANSCRIPT:\n",
    "Now we are engaged in a great civil war...\n",
    "**###** RESPONSE:\n",
    "and prints the generated output.\n",
    "\n",
    "This is strictly a quick test to show proof-of-concept, so prioritize speed and simplicity. The goal is just to show the model responds to the fine-tuning data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc556059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22591266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EXCEL_FILE = r\"../../data/mccray/changed_data/decade_subsets/McCray (1940s, messy_count=0).xlsx\"  # Replace with your file path\n",
    "MODEL_NAME = \"distilgpt2\"  # Very small model for CPU training\n",
    "# Alternative for slightly better quality: \"microsoft/DialoGPT-small\"\n",
    "OUTPUT_DIR = \"./lora-finetuned-test-model-v2\"\n",
    "MAX_LENGTH = 256  # Shorter for CPU efficiency\n",
    "MIN_TRANSCRIPT_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d413190c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LoRA Fine-tuning Pipeline\n",
      "==================================================\n",
      "Loading Excel file...\n",
      "Original dataset size: 746\n",
      "After filtering: 725\n",
      "Creating HuggingFace Dataset...\n",
      "Loading model and tokenizer: distilgpt2\n",
      "Setting up LoRA...\n",
      "trainable params: 202,752 || all params: 82,115,328 || trainable%: 0.2469\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 725/725 [00:00<00:00, 3030.98 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:24, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.874400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6.993800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.204100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.469600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.705800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.256300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.074000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.339900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING GENERATION\n",
      "==================================================\n",
      "Input prompt:\n",
      "### TITLE: Lincoln's Gettysburg Address\n",
      "### TRANSCRIPT:\n",
      "Now we are engaged in a great civil war, testing whether that nation or any nation so conceived and so dedicated can long endure.\n",
      "### RESPONSE:\n",
      "\n",
      "Generated response:\n",
      "\n",
      "\n",
      "Training completed! Model saved to: ./lora-finetuned-test-model\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_data(excel_file):\n",
    "    \"\"\"Load Excel file and preprocess the data\"\"\"\n",
    "    print(\"Loading Excel file...\")\n",
    "    df = pd.read_excel(excel_file)\n",
    "    \n",
    "    print(f\"Original dataset size: {len(df)}\")\n",
    "    \n",
    "    # Drop rows with missing or short content\n",
    "    df = df.dropna(subset=['Title', 'Transcript'])\n",
    "    df = df[df['Title'].str.len() > 0]\n",
    "    df = df[df['Transcript'].str.len() >= MIN_TRANSCRIPT_LENGTH]\n",
    "    \n",
    "    print(f\"After filtering: {len(df)}\")\n",
    "    \n",
    "    # Create training format\n",
    "    def format_example(title, transcript):\n",
    "        prompt = f\"### TITLE: {title}\\n### TRANSCRIPT:\\n{transcript}\\n### RESPONSE:\"\n",
    "        response = transcript  # Using transcript as response for demo\n",
    "        return f\"{prompt} {response}\"\n",
    "    \n",
    "    df['text'] = df.apply(lambda row: format_example(row['Title'], row['Transcript']), axis=1)\n",
    "    \n",
    "    return df[['text']]\n",
    "\n",
    "def create_dataset(df):\n",
    "    \"\"\"Convert DataFrame to HuggingFace Dataset\"\"\"\n",
    "    print(\"Creating HuggingFace Dataset...\")\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    return dataset\n",
    "\n",
    "def setup_model_and_tokenizer(model_name):\n",
    "    \"\"\"Load model and tokenizer for CPU training\"\"\"\n",
    "    print(f\"Loading model and tokenizer: {model_name}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load model for CPU\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float32,  # Use float32 for CPU\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def setup_lora(model):\n",
    "    \"\"\"Configure LoRA for the model\"\"\"\n",
    "    print(\"Setting up LoRA...\")\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=4,  # Even lower rank for CPU training\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"c_attn\", \"c_proj\"],  # For GPT-2 based models\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer):\n",
    "    \"\"\"Tokenize the dataset and add labels for Causal LM\"\"\"\n",
    "    print(\"Tokenizing dataset...\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        tokens = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_LENGTH,\n",
    "        )\n",
    "        # Add labels = input_ids for causal LM loss\n",
    "        tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "        return tokens\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "def train_model(model, tokenizer, train_dataset):\n",
    "    \"\"\"Fine-tune the model using Trainer\"\"\"\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=2,  # Smaller for CPU\n",
    "        warmup_steps=5,\n",
    "        max_steps=20,  # Very limited for CPU demo\n",
    "        learning_rate=1e-3,  # Higher learning rate for faster convergence\n",
    "        logging_steps=2,\n",
    "        save_strategy=\"epoch\",\n",
    "        eval_strategy=\"no\",\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        no_cuda=True,  # Force CPU usage\n",
    "        dataloader_num_workers=0,  # Avoid multiprocessing issues\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the model\n",
    "    print(\"Saving model...\")\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "def test_generation(model, tokenizer):\n",
    "    \"\"\"Test the fine-tuned model with a sample prompt\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TESTING GENERATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    test_prompt = \"\"\"### TITLE: Lincoln's Gettysburg Address\n",
    "### TRANSCRIPT:\n",
    "Now we are engaged in a great civil war, testing whether that nation or any nation so conceived and so dedicated can long endure.\n",
    "### RESPONSE:\"\"\"\n",
    "    \n",
    "    print(f\"Input prompt:\\n{test_prompt}\\n\")\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate response\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=50,  # Shorter for CPU\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode and print\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response_start = generated_text.find(\"### RESPONSE:\") + len(\"### RESPONSE:\")\n",
    "    response = generated_text[response_start:].strip()\n",
    "    \n",
    "    print(f\"Generated response:\\n{response}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    print(\"Starting LoRA Fine-tuning Pipeline\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        # 1. Load and preprocess data\n",
    "        df = load_and_preprocess_data(EXCEL_FILE)\n",
    "        \n",
    "        # 2. Create dataset\n",
    "        dataset = create_dataset(df)\n",
    "        \n",
    "        # 3. Setup model and tokenizer\n",
    "        model, tokenizer = setup_model_and_tokenizer(MODEL_NAME)\n",
    "        \n",
    "        # 4. Setup LoRA\n",
    "        model = setup_lora(model)\n",
    "        \n",
    "        # 5. Tokenize dataset\n",
    "        train_dataset = tokenize_dataset(dataset, tokenizer)\n",
    "        \n",
    "        # 6. Train model\n",
    "        trainer = train_model(model, tokenizer, train_dataset)\n",
    "        \n",
    "        # 7. Test generation\n",
    "        test_generation(model, tokenizer)\n",
    "        \n",
    "        print(f\"\\nTraining completed! Model saved to: {OUTPUT_DIR}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        print(\"Make sure you have the required packages installed:\")\n",
    "        print(\"pip install torch transformers datasets peft pandas openpyxl\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25905ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-5): 6 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=2304, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=768, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D(nf=3072, nx=768)\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=768, nx=3072)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the model\n",
    "\n",
    "# Path to saved model\n",
    "output_dir = \"./lora-finetuned-test-model\"\n",
    "\n",
    "# Load PEFT config\n",
    "peft_config = PeftConfig.from_pretrained(output_dir)\n",
    "\n",
    "# Load base model and tokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Attach LoRA weights\n",
    "model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "\n",
    "# Set model to eval mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38baf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TITLE: John Henry McCray, business card\n",
      "### TRANSCRIPT:\n",
      "Office   1507   HARDIN  ST. Hama:   2018 ...\n",
      "### RESPONSE:\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"\"\"### TITLE: In the Court of General Sessions, June 19, 1950\n",
    "### TRANSCRIPT:\n",
    "STATE OF SOUTH CAROLINA    COUNTY OF NEWBERRY    IN THE COURT OF GENERAL SESSIONS    On June 19, 1950, John McCray, editor of The Lighthouse  and Informer, appeared at the Court of General Sessions,  Newberry, South Carolina, and pled guilty to a charge against  him of criminal libel, that had been preferred by the Grant  Jury of Greenwood County, and was given the following    sentence\n",
    "### RESPONSE:\"\"\"\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6521afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Models can be pushed to huggingface when we have a model that we want to do further testing on\n",
    "\n",
    "from huggingface_hub import login\n",
    "from peft import PeftModel\n",
    "\n",
    "model.push_to_hub(\"your-username/your-model-name\")\n",
    "tokenizer.push_to_hub(\"your-username/your-model-name\")\n",
    "\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_aspire)",
   "language": "python",
   "name": "venv_aspire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
