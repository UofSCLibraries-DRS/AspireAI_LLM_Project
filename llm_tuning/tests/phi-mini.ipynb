{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32dd3b1c",
   "metadata": {},
   "source": [
    "# Phi 3 mini test (microsoft llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e113ffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steven-dindl/Documents/AspireAI_LLM_Project/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-3.5-mini-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-3.5-mini-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Fetching 2 files: 100%|██████████| 2/2 [00:35<00:00, 17.89s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"microsoft/phi-3.5-mini-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Function to split text into chunks\n",
    "def chunk_text(text, max_words=500):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_words):\n",
    "        chunks.append(\" \".join(words[i:i+max_words]))\n",
    "    return chunks\n",
    "\n",
    "# Function to create a strict no-paraphrasing prompt\n",
    "def create_no_rephrase_prompt(text_chunk):\n",
    "    return f\"\"\"\n",
    "You are a meticulous text proofreader. \n",
    "Correct ONLY spelling, punctuation, and grammatical errors. \n",
    "Do NOT change wording, sentence structure, or style unless absolutely necessary. \n",
    "Preserve all original text, tone, and sentence order.\n",
    "Even if a sentence is incomplete, it is importance that we keep the original document intact.\n",
    "Output only the text without any additional statement\n",
    "\n",
    "Text:\n",
    "\\\"\\\"\\\"{text_chunk}\\\"\\\"\\\"\n",
    "\n",
    "Corrected Text:\n",
    "\"\"\"\n",
    "\n",
    "# Function to correct a single chunk\n",
    "def correct_chunk(chunk):\n",
    "    prompt = create_no_rephrase_prompt(chunk)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1000,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    corrected_text = tokenizer.decode(output_ids[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "    return corrected_text.strip()\n",
    "\n",
    "# Main function to correct full transcript\n",
    "def correct_transcript(text):\n",
    "    chunks = chunk_text(text)\n",
    "    corrected_chunks = []\n",
    "    for chunk in tqdm(chunks, desc=\"Correcting transcript\"):\n",
    "        corrected_chunks.append(correct_chunk(chunk))\n",
    "    return \" \".join(corrected_chunks)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your transcript from a file\n",
    "    with open(\"transcript.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        transcript = f.read()\n",
    "\n",
    "    corrected_transcript = correct_transcript(transcript)\n",
    "\n",
    "    # Save corrected transcript\n",
    "    with open(\"transcript_corrected.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(corrected_transcript)\n",
    "\n",
    "    print(\"Transcript correction complete. Saved to transcript_corrected.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f910d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_aspire)",
   "language": "python",
   "name": "venv_aspire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
