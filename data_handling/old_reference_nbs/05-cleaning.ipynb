{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a9edd4f",
   "metadata": {},
   "source": [
    "# Cleaning with LLMs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf71f55",
   "metadata": {},
   "source": [
    "https://huggingface.co/unsloth/Phi-3-medium-4k-instruct?utm_source=chatgpt.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dd7004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steven-dindl/Documents/AspireAI_LLM_Project/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Load the model and necessary libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the text generation pipeline with Phi-3\n",
    "pipe = pipeline(\"text-generation\", model=\"unsloth/Phi-3-medium-4k-instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670cd52c",
   "metadata": {},
   "source": [
    "## OCR Text Cleaning Pipeline\n",
    "\n",
    "We'll create a specialized OCR correction pipeline using the Phi-3 language model with a carefully crafted prompt designed to:\n",
    "\n",
    "1. Fix OCR artifacts and errors while preserving the original text's style and meaning\n",
    "2. Avoid hallucinations or unnecessary rewrites\n",
    "3. Flag ambiguous text rather than guessing\n",
    "4. Preserve original formatting, punctuation, and capitalization\n",
    "5. Process text in a pandas DataFrame efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5892c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our specialized OCR correction prompt\n",
    "def create_ocr_correction_prompt(text):\n",
    "    prompt = \"\"\"<|system|>\n",
    "You are a meticulous text restorer. Your job is to correct OCR artifacts in legal or historical documents.\n",
    "\n",
    "RULES:\n",
    "1. Do NOT rewrite or modernize wording — preserve original grammar, style, and vocabulary, even if unusual.\n",
    "2. Only fix:\n",
    "   - Obvious misspellings caused by OCR (e.g., \"oi\" → \"of\", \"xkxi\" → remove or replace with intended text if 100% clear from context).\n",
    "   - Broken words due to incorrect character recognition.\n",
    "   - Spacing issues.\n",
    "3. If a word is unclear or ambiguous, do NOT guess. Keep it as-is but flag it in [square brackets].\n",
    "4. Preserve punctuation, capitalization, and formatting exactly as in the original unless it is clearly an OCR error.\n",
    "5. Never add or remove whole sentences or change meaning.\n",
    "6. Output only the corrected text — no explanations.\n",
    "<|user|>\n",
    "TEXT TO CORRECT:\n",
    "\"\"\"\n",
    "{}\n",
    "\"\"\"\n",
    "<|assistant|>\n",
    "\"\"\".format(text)\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de04631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to correct OCR text using the LLM\n",
    "def correct_ocr_text(text, max_length=4000):\n",
    "    \"\"\"\n",
    "    Clean OCR text using our specialized LLM prompt\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        The OCR text to clean\n",
    "    max_length : int\n",
    "        Maximum length for text generation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str: Corrected text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return text\n",
    "        \n",
    "    # Prepare prompt with the text\n",
    "    prompt = create_ocr_correction_prompt(text)\n",
    "    \n",
    "    # Generate corrected text\n",
    "    result = pipe(\n",
    "        prompt,\n",
    "        max_length=max_length,\n",
    "        do_sample=False,  # Deterministic generation for OCR correction\n",
    "        temperature=0.1,  # Very low temperature for minimal creativity\n",
    "        return_full_text=False\n",
    "    )[0][\"generated_text\"]\n",
    "    \n",
    "    # Clean up the output - remove any leading/trailing quotes and whitespace\n",
    "    cleaned_result = result.strip().strip('\"\"\"').strip()\n",
    "    \n",
    "    return cleaned_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd4774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame processor function that applies OCR correction to specified columns\n",
    "def process_dataframe_ocr(df, text_columns, batch_size=10, show_progress=True):\n",
    "    \"\"\"\n",
    "    Process a DataFrame by correcting OCR errors in specified text columns\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        DataFrame containing the text to process\n",
    "    text_columns : list or str\n",
    "        Column name(s) containing OCR text to correct\n",
    "    batch_size : int\n",
    "        Number of rows to process in each batch for progress tracking\n",
    "    show_progress : bool\n",
    "        Whether to show progress information\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame: DataFrame with corrected text columns\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Convert text_columns to list if it's a string\n",
    "    if isinstance(text_columns, str):\n",
    "        text_columns = [text_columns]\n",
    "    \n",
    "    # Total number of rows to process\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # Process data in batches\n",
    "    for col in text_columns:\n",
    "        if show_progress:\n",
    "            print(f\"Processing column: {col}\")\n",
    "        \n",
    "        # Ensure the column exists\n",
    "        if col not in df.columns:\n",
    "            print(f\"Warning: Column '{col}' not found in DataFrame. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Create a new column for the cleaned text\n",
    "        cleaned_col = f\"{col}_cleaned\"\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, total_rows, batch_size):\n",
    "            batch_end = min(i + batch_size, total_rows)\n",
    "            \n",
    "            if show_progress:\n",
    "                print(f\"  Processing rows {i+1} to {batch_end} of {total_rows}...\")\n",
    "            \n",
    "            # Process each row in the current batch\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for j in range(i, batch_end):\n",
    "                # Get the text for this row\n",
    "                text = df.iloc[j][col]\n",
    "                \n",
    "                # Only process if it's a valid string\n",
    "                if isinstance(text, str) and text.strip():\n",
    "                    # Apply OCR correction\n",
    "                    result_df.loc[df.index[j], cleaned_col] = correct_ocr_text(text)\n",
    "                else:\n",
    "                    result_df.loc[df.index[j], cleaned_col] = text\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            if show_progress:\n",
    "                print(f\"  Batch completed in {elapsed:.2f} seconds\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba15586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze differences between original and corrected text\n",
    "def analyze_corrections(original, corrected):\n",
    "    \"\"\"\n",
    "    Analyze the differences between original and corrected text\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original : str\n",
    "        Original OCR text\n",
    "    corrected : str\n",
    "        Corrected text from the LLM\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Dictionary with metrics about the corrections\n",
    "    \"\"\"\n",
    "    if not isinstance(original, str) or not isinstance(corrected, str):\n",
    "        return {\n",
    "            'original_words': 0,\n",
    "            'corrected_words': 0,\n",
    "            'different_words': 0,\n",
    "            'percent_changed': 0,\n",
    "            'flagged_words': 0\n",
    "        }\n",
    "    \n",
    "    # Tokenize into words\n",
    "    original_words = re.findall(r'\\b\\w+\\b', original.lower())\n",
    "    corrected_words = re.findall(r'\\b\\w+\\b', corrected.lower())\n",
    "    \n",
    "    # Count words in square brackets (flagged as ambiguous)\n",
    "    flagged_count = len(re.findall(r'\\[.*?\\]', corrected))\n",
    "    \n",
    "    # Count different words\n",
    "    orig_set = set(original_words)\n",
    "    corr_set = set(corrected_words)\n",
    "    \n",
    "    # Words in corrected that aren't in original\n",
    "    different = len(corr_set - orig_set)\n",
    "    \n",
    "    # Calculate percentage changed\n",
    "    if len(orig_set) > 0:\n",
    "        pct_changed = (different / len(orig_set)) * 100\n",
    "    else:\n",
    "        pct_changed = 0\n",
    "        \n",
    "    return {\n",
    "        'original_words': len(orig_set),\n",
    "        'corrected_words': len(corr_set),\n",
    "        'different_words': different,\n",
    "        'percent_changed': pct_changed,\n",
    "        'flagged_words': flagged_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7760d2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate correction metrics for the entire DataFrame\n",
    "def generate_correction_metrics(df, original_cols, corrected_cols):\n",
    "    \"\"\"\n",
    "    Generate metrics about OCR corrections across a DataFrame\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        DataFrame containing original and corrected text\n",
    "    original_cols : list or str\n",
    "        Column name(s) containing original OCR text\n",
    "    corrected_cols : list or str\n",
    "        Column name(s) containing corrected OCR text\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame: DataFrame with correction metrics\n",
    "    \"\"\"\n",
    "    # Convert to lists if strings\n",
    "    if isinstance(original_cols, str):\n",
    "        original_cols = [original_cols]\n",
    "    if isinstance(corrected_cols, str):\n",
    "        corrected_cols = [corrected_cols]\n",
    "        \n",
    "    # Check lengths match\n",
    "    if len(original_cols) != len(corrected_cols):\n",
    "        raise ValueError(\"Number of original columns must match number of corrected columns\")\n",
    "    \n",
    "    # Create a DataFrame to store metrics\n",
    "    metrics_df = pd.DataFrame()\n",
    "    \n",
    "    # Process each column pair\n",
    "    for orig_col, corr_col in zip(original_cols, corrected_cols):\n",
    "        # Check columns exist\n",
    "        if orig_col not in df.columns or corr_col not in df.columns:\n",
    "            print(f\"Warning: Columns '{orig_col}' and/or '{corr_col}' not found. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        # Apply analysis to each row\n",
    "        metrics = []\n",
    "        for _, row in df.iterrows():\n",
    "            metric = analyze_corrections(row[orig_col], row[corr_col])\n",
    "            metrics.append(metric)\n",
    "            \n",
    "        # Convert to DataFrame\n",
    "        col_metrics = pd.DataFrame(metrics)\n",
    "        \n",
    "        # Add column identifiers\n",
    "        col_metrics['original_column'] = orig_col\n",
    "        col_metrics['corrected_column'] = corr_col\n",
    "        \n",
    "        # Append to main metrics DataFrame\n",
    "        metrics_df = pd.concat([metrics_df, col_metrics], ignore_index=True)\n",
    "    \n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb19edfb",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Here's an example of how to use the OCR correction pipeline on a sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd782188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a sample DataFrame with OCR errors\n",
    "sample_data = {\n",
    "    'id': [1, 2, 3],\n",
    "    'title': ['Document A', 'Document B', 'Document C'],\n",
    "    'transcript': [\n",
    "        \"Tke Board oi Directors met on Aprii 15, 2021 to discusa tke annual oudget. Mr. Smitk presented tke financial reports whick showed an increased prolit of 15% compared to last yéar.\",\n",
    "        \"Purshant to Section 8.2 oi the agreement, the partles hereby agtee to exfend the Term for an additlonal period oi five (5) gears, commencing on January 1, 2022.\",\n",
    "        \"Withm 30 deys of recerpt, all invoiccs must be processéd by the Acconnts department and forwarded t0 the Finance Director f0r approval.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "sample_df = pd.DataFrame(sample_data)\n",
    "\n",
    "# Display the original DataFrame\n",
    "print(\"Original Sample Data:\")\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c962d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply OCR correction to the 'transcript' column\n",
    "# Note: This will take some time depending on your hardware and model size\n",
    "# Using a small batch size for this example\n",
    "corrected_df = process_dataframe_ocr(sample_df, 'transcript', batch_size=1)\n",
    "\n",
    "# Display the corrected DataFrame\n",
    "print(\"\\nCorrected Sample Data:\")\n",
    "corrected_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze corrections made by the model\n",
    "corrections_analysis = analyze_corrections(sample_df['transcript'], corrected_df['transcript'])\n",
    "\n",
    "# Print the analysis results\n",
    "print(\"Correction Analysis:\")\n",
    "print(f\"Total corrections made: {corrections_analysis['total_corrections']}\")\n",
    "print(f\"Average corrections per text: {corrections_analysis['avg_corrections_per_text']:.2f}\")\n",
    "print(f\"Character change rate: {corrections_analysis['char_change_rate']:.2%}\")\n",
    "print(f\"Longest correction: {corrections_analysis['longest_correction']}\")\n",
    "print(\"\\nCommon corrections:\")\n",
    "for correction, count in corrections_analysis['common_corrections'].items():\n",
    "    print(f\"'{correction[0]}' → '{correction[1]}': {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d82e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a side-by-side comparison DataFrame for visualization\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original Text': sample_df['Transcript'],\n",
    "    'Corrected Text': corrected_df['Transcript']\n",
    "})\n",
    "\n",
    "# Function to highlight differences between original and corrected text\n",
    "def highlight_differences(s):\n",
    "    original = s['Original Text']\n",
    "    corrected = s['Corrected Text']\n",
    "    \n",
    "    # Basic diff visualization (for more complex highlighting, consider libraries like difflib)\n",
    "    if original != corrected:\n",
    "        return ['background-color: #ffcccc', 'background-color: #ccffcc']\n",
    "    else:\n",
    "        return ['', '']\n",
    "\n",
    "# Display the comparison with highlighting\n",
    "print(\"\\nSide-by-Side Comparison (Red: Original, Green: Corrected):\")\n",
    "comparison_df.style.apply(highlight_differences, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and modify the path as needed\n",
    "output_path = '../data/mccray/changed_data/sample_corrected.xlsx'\n",
    "\n",
    "df.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbf7063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load real data from Excel files\n",
    "def load_mccray_data(file_path):\n",
    "    \"\"\"\n",
    "    Load data from McCray Excel files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the Excel file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        Loaded data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        print(f\"Loaded {len(df)} rows from {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example: Process a small subset of real data\n",
    "# Uncomment and modify as needed to process real data\n",
    "\n",
    "\"\"\"\n",
    "# Path to a decade subset or other McCray data file\n",
    "real_data_path = '../../data/mccray/changed_data/decade_subsets/McCray (1940s, 100 rows).xlsx'\n",
    "\n",
    "# Load the data\n",
    "real_df = load_mccray_data(real_data_path)\n",
    "\n",
    "if real_df is not None and 'transcript' in real_df.columns:\n",
    "    # Check how many rows have transcripts\n",
    "    has_transcript = real_df['transcript'].notna()\n",
    "    print(f\"Rows with transcripts: {has_transcript.sum()} of {len(real_df)}\")\n",
    "    \n",
    "    # Process only rows that have transcripts (limit to first 10 for testing)\n",
    "    df_to_process = real_df[has_transcript].head(10).copy()\n",
    "    \n",
    "    if len(df_to_process) > 0:\n",
    "        # Process the data with the LLM\n",
    "        print(\"Processing real data with the OCR correction pipeline...\")\n",
    "        corrected_real_df = process_dataframe_ocr(df_to_process, 'transcript', batch_size=2)\n",
    "        \n",
    "        # Analyze the corrections\n",
    "        real_corrections = analyze_corrections(df_to_process['transcript'], corrected_real_df['transcript'])\n",
    "        \n",
    "        print(\"\\nReal Data Correction Analysis:\")\n",
    "        print(f\"Total corrections made: {real_corrections['total_corrections']}\")\n",
    "        print(f\"Average corrections per text: {real_corrections['avg_corrections_per_text']:.2f}\")\n",
    "        \n",
    "        # Save the corrected data\n",
    "        output_path = '../../data/mccray/changed_data/llm_corrected/mccray_1940s_sample_corrected.xlsx'\n",
    "        save_corrected_data(corrected_real_df, output_path)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945319f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance optimization and large dataset processing\n",
    "\"\"\"\n",
    "# Tips for optimizing the OCR correction pipeline:\n",
    "\n",
    "1. Process Large Datasets in Chunks:\n",
    "\n",
    "# Function to process a large dataset in batches\n",
    "def process_large_dataset(input_path, output_path, text_column='transcript', chunk_size=100, batch_size=5):\n",
    "    \"\"\"\n",
    "    Process a large dataset in chunks to avoid memory issues.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_path : str\n",
    "        Path to the input Excel file\n",
    "    output_path : str\n",
    "        Path to save the output Excel file\n",
    "    text_column : str\n",
    "        Column name containing the text to correct\n",
    "    chunk_size : int\n",
    "        Number of rows to process in each chunk\n",
    "    batch_size : int\n",
    "        Batch size for the LLM processing within each chunk\n",
    "    \"\"\"\n",
    "    # Load the dataset in chunks\n",
    "    reader = pd.read_excel(input_path, chunksize=chunk_size)\n",
    "    \n",
    "    # Process each chunk\n",
    "    all_processed_chunks = []\n",
    "    chunk_counter = 0\n",
    "    \n",
    "    for chunk in reader:\n",
    "        chunk_counter += 1\n",
    "        print(f\"Processing chunk {chunk_counter} ({len(chunk)} rows)\")\n",
    "        \n",
    "        # Filter rows with text\n",
    "        has_text = chunk[text_column].notna()\n",
    "        to_process = chunk[has_text].copy()\n",
    "        no_text = chunk[~has_text].copy()\n",
    "        \n",
    "        if len(to_process) > 0:\n",
    "            # Process the chunk\n",
    "            processed_chunk = process_dataframe_ocr(to_process, text_column, batch_size=batch_size)\n",
    "            # Combine with rows that didn't have text\n",
    "            result_chunk = pd.concat([processed_chunk, no_text])\n",
    "        else:\n",
    "            result_chunk = chunk\n",
    "            \n",
    "        all_processed_chunks.append(result_chunk)\n",
    "        print(f\"Completed chunk {chunk_counter}\")\n",
    "    \n",
    "    # Combine all processed chunks\n",
    "    final_df = pd.concat(all_processed_chunks)\n",
    "    \n",
    "    # Save the final result\n",
    "    save_corrected_data(final_df, output_path)\n",
    "    print(f\"Complete dataset processed and saved to {output_path}\")\n",
    "\n",
    "2. Use GPU acceleration:\n",
    "   - If available, ensure the model is loaded on GPU using device='cuda'\n",
    "   - For multi-GPU setups, consider using device_map='auto'\n",
    "\n",
    "3. Optimize memory usage:\n",
    "   - Set torch_dtype=torch.float16 for half precision\n",
    "   - Use 8-bit quantization: load_in_8bit=True\n",
    "   - Consider using Unsloth for optimized models: https://github.com/unslothai/unsloth\n",
    "\n",
    "4. For extremely large datasets:\n",
    "   - Consider saving intermediate results after each chunk\n",
    "   - Implement checkpointing to resume processing if interrupted\n",
    "   - Use a distributed processing approach with multiple machines\n",
    "\n",
    "5. For batch processing multiple files:\n",
    "   - Create a list of files and process them sequentially\n",
    "   - Implement parallel processing using multiprocessing or joblib\n",
    "\n",
    "# Example usage for large dataset processing:\n",
    "# process_large_dataset(\n",
    "#     input_path='../../data/mccray/changed_data/McCray+.xlsx',\n",
    "#     output_path='../../data/mccray/changed_data/llm_corrected/McCray+_corrected.xlsx',\n",
    "#     chunk_size=100,\n",
    "#     batch_size=5\n",
    "# )\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_aspire)",
   "language": "python",
   "name": "venv_aspire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
